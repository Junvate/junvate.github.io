---
layout:     post
title:      项目复习
subtitle:   项目复习
date:       2025-05-10
author:     Junvate
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 保研复习
---


# 项目
## 速记
该项目核心采用了检索增强生成（RAG）技术架构，旨在提升大型语言模型（LLM）在处理专业文档（如技术博客）问答时的准确性和时效性。其技术实现主要包括：利用嵌入模型（Embedding Models）将文档内容转化为向量表示（Vector Embeddings），并存储于向量数据库（Vector Database）中以实现高效索引；当用户提问时，通过语义搜索（Semantic Search）算法（如余弦相似度）在向量库中匹配并检索出最相关的文本片段。接着，关键的整合与生成步骤由 LangChain 框架进行流程编排（Orchestration），它负责将检索到的上下文信息与用户问题组合成有效的提示（Prompt），并调用大型语言模型（LLM，如 GPT-3.5-Turbo 或类似模型）生成回答。同时，LangChain 也被用来实现对话记忆（Memory）和状态管理。最后，项目使用 Ragas 评估框架对 RAG 系统的性能（如答案的忠实度和相关性）进行量化评估，验证了相比基础 LLM 精准度提升 40% 的技术效果。


这个项目旨在预测在特定提示（prompt）下，用户更偏好两个语言模型响应（response_a 或 response_b）中的哪一个，这是一个三分类问题（A赢、B赢或平局）。其核心技术是基于强大的预训练模型 DeBERTa V3 进行模型微调 (Model Fine-tuning)项目首先加载 CSV 数据，使用 eval 处理列表式字符串，填充缺失值，并通过 idxmax 在 winner_model_a, winner_model_b, winner_tie 列上确定优胜者，然后将其映射为数字标签 class_id（0, 1, 2）。关键的预处理步骤是将提示与每个响应按特定格式拼接（使用 f-string 如 f"{prompt}\n\nResponse A:\n{response_a}"），为每个样本生成一对包含上下文的响应文本。接着，使用 KerasNLP 提供的  deberta_v3 将这些文本对 tokenize 成包含 token_ids 和 padding_mask 的字典，其形状均为 [2, 512]。
模型构建阶段 (build_model 函数)：模型接收这个字典输入，在内部分离处理两个序列（prompt+A 和 prompt+B），分别将它们送入 DebertaV3 模型。利用模型的自注意力机制，提取每个序列对应的 [CLS] 输出嵌入 。随后，将这两个 [CLS] 嵌入向量堆叠 (tf.stack) 起来，并通过 GlobalAveragePooling1D 层进行处理（这实际上是平均了两个 [CLS] 嵌入向量）。最后，将这个池化后的向量输入到一个包含3个神经元、使用 Softmax 激活函数的全连接（Dense）输出层，得到三分类的概率预测。训练过程中，模型使用分类交叉熵 (CategoricalCrossentropy) 作为损失函数进行编译，采用了自定义的带预热的余弦衰减学习率调度器 (WarmUpCosineDecay)，并通过 ModelCheckpoint 回调函数监测验证集损失 (val_loss) 来保存性能最佳的模型权重。
从而将 DeBERTa V3的通用语言理解能力适配到这个特定的偏好预测任务上。


我独立开发并部署了一套基于YOLOv10的智慧工地安全监控系统，旨在通过实时智能监测工人安全帽和反光衣的佩戴情况，显著提升建筑工地的安全管理效率和水平。在该项目中，我负责了从前期数据采集、清洗与标注，到核心的YOLOv10目标检测模型的构建、训练与深度优化（在自建测试集上实现了91.4%的mAP50指标）的全过程。为解决实际场景中小目标检测准确率不足的问题，我专门设计并实施了多尺度图像增强方案。此外，我还利用OpenCV完成了视频流的实时捕获与图像预处理工作，并开发了与检测结果联动的自动告警系统，最终确保了整个系统在生产环境中的稳定运行和有效预警。


## RAG
### 什么是RAG 讲一下工作原理？
RAG（Retrieval-Augmented Generation，检索增强生成）是一种结合了信息检索和生成式语言模型的混合框架，旨在提升大语言模型（LLM）在回答问题时的准确性、相关性和知识时效性。它通过从外部知识库检索相关信息并将其融入生成过程，有效解决LLM的幻觉问题（生成不准确或虚构内容）和知识局限性（无法获取最新或专业知识）。RAG特别适用于需要引用外部文档或处理专业领域知识的场景

RAG的工作原理:  
RAG的核心思想是将信息检索与生成式模型结合，分为两个主要阶段：  
检索阶段：从外部知识库中检索与用户查询最相关的文档或片段，作为生成阶段的上下文。  
生成阶段：将检索到的文档与用户查询结合，输入到生成式语言模型中，生成最终的回答。

### 你这个评估框架Ragas是怎么进行评估的？
- Ragas框架提供多个衡量评价RAG系统的指标包括
- failfulness忠实度：是否是基于上下文进行回答的，这是衡量模型是否产生幻觉的一个标准
- answer relevance相关度：回答的问题是否和问题相关，它会评估答案是否集中于解决问题的核心。
- context precision上下文精准度:衡量所有检索到的上下文中，和答案真正有关系的比例
- context recall上下文召回率：衡量所有与答案有关的上下文中，有多少被检索系统成功找回来


### 语义相似性从用户提供的网页文档中检索信息”，具体是如何实现的？使用了什么嵌入模型（Embedding Model）将文本转换为向量？为什么选择这款嵌入模型？
从用户提供的网页文档中通过语义相似性检索信息时，
其实现过程通常是：首先，系统会抓取并解析网页内容，提取出主要的文本信息，去除HTML标签、广告、导航栏等无关元素；接着，这些文本会被送入一个嵌入模型（Embedding Model）转换为高维向量。例如，我可能会选择使用像 text-embedding-3-small (来自OpenAI) 这样的先进模型，
或者一个强大的开源模型如 sentence-transformers/all-mpnet-base-v2。选择这类模型的原因在于它们在各类语义理解任务上表现出色，能够生成捕捉文本深层含义的向量，并且在性能、成本（对于API模型）和易用性之间取得了良好平衡，确保了后续相似性搜索的准确性。


### 检索时，你是如何定义和衡量“语义相似性”的？通常会检索多少个相关的文本块（top-k）？这个k值是如何确定的？
在检索时，“语义相似性”通常是通过计算查询文本向量与文档块向量之间的余弦相似度 (Cosine Similarity) 来定义和衡量的；
对于归一化后的向量，内积也可以达到同样的效果。
余弦相似度能够很好地捕捉向量方向上的一致性，即语义上的接近程度，而不受向量长度的影响。
通常会检索 top-k 个最相关的文本块，k 的值一般会根据经验和实验来确定，常见的范围是 3到5个。
这个k值的确定需要权衡多个因素：一方面要确保提供给LLM足够的信息以回答问题，
另一方面要避免引入过多不相关或冗余的信息，这会增加LLM的处理负担并可能降低答案质量。
通过在验证集上测试不同的k值，并评估最终问答的准确率、Ragas指标（如Faithfulness, Context Recall）等，
可以找到一个最优的k值。



### 如果让你继续优化这个系统，你有哪些改进的想法？（例如，引入更先进的检索策略、支持更复杂的查询、优化Prompt工程、尝试不同的LLM等）






## Yolo
### 什么是Yolo
- YOLO（You Only Look Once）是一种基于深度学习的目标检测算法，它通过将目标检测任务转化为单一的回归问题，
直接从输入图像预测边界框（bounding box）和类别概率，实现了端到端的检测。

网格划分：YOLO将输入图像划分为S×S的网格，每个网格负责检测其覆盖区域内的目标。  
预测内容：每个网格预测多个边界框（包括坐标、宽高）、置信度（表示框内是否有目标）以及类别概率。  
端到端训练：通过卷积神经网络（CNN）一次性处理整个图像，输出所有检测结果。  
后处理：使用非极大值抑制（NMS）去除重叠框，保留最佳检测结果。

### 为什么你要选择yolo而不是其他算法
- yolov10是一个基于深度学习的单阶段目标检测算法，在**速度检测和效率**上更有优势，对于这种实时处理的部署场景有更大的优势，
- 相对于第八代和第九代，通过无NMS和架构优化优先考虑端到端的实时性能，更注重极致的效率
- 相对于Faster R-CNN ,它是一种双阶段目标检测器，首先进行区域提议（region proposal），然后再对这些提议进行分类和边界框回归
Faster R-CNN以其较高的检测精度闻名，尤其是在处理小目标或复杂场景时，但是yolov10在速度和精度的平衡上会更好
YOLOv10的NMS-free特性进一步拉大了速度差距。
##### 介绍一下NMS
在传统的目标检测中对一个物体的检测常常有多个候选框(bounding boxes)，会有置信度分数，NMS的作用就是**筛选最佳的候选框**
**抑制冗余的候选框**，但是NMS是一个计算的开销增加推理时间
##### 介绍一下NMS-free
- 核心思想是 改进的标签分配策略（Label Assignment）







### 你提到mAP50指标达到91.4%，请解释mAP50的计算方法及其在目标检测中的意义。
- 它的全称是 "mean Average Precision at an IoU threshold of 0.5
- mAP50是目标检测中的重要评估指标，表示在IoU（交并比）阈值为0.5时的平均精度。
- 对每个类别，计算预测结果的精度-召回曲线（Precision-Recall Curve）。计算每条曲线的曲线下面积（AUC）对所有类别的AUC取平均值，得到mAP50。
- mAP50的意义在于衡量模型在不同类别上的检测性能，91.4%表明模型在IoU≥0.5时具有较高的准确性。
- precision精度 衡量的是所有被模型检测出来的有多少是正例
- recall召回率 衡量得失有多少真正的正例被检测出来
- Average Precision就是这条 P-R 曲线下的面积。高 Precision 低 Recall 意味着模型检测到的结果大多是正确的，但漏掉了很多物体。高 Recall 低 Precision 意味着模型检测出了大部分物体，但很多检测结果是错误的。
- F1-score  是precision和recall的调和平均数 = (Precision×Recall)/(Precision+Recall)  它在两者都较高时才会取得高分。




## DeBERTa V3
### DeBERTa V3是什么
- DeBERTa V3 是由微软研究院开发的一种预训练自然语言处理（NLP）模型。

- DeBERTa V3 是一个通过采用更高效的 ELECTRA 式预训练方法（RTD）显著提升了训练效率和性能的 Transformer 语言模型。它在自然语言理解任务上表现强大

DeBERTa V3 模型会对输入的 token IDs 和 padding mask 进行嵌入（embedding）、位置编码（positional encoding）和多层自注意力机制（self-attention）等操作，最终生成每个 token 的上下文感知的嵌入表示

backbone 的作用: DebertaV3Backbone 这样的模型通常用来将输入的 token ID 序列转换成上下文相关的词嵌入 (contextualized embeddings) 或称为隐藏状态 (hidden states)。简单来说，它为输入序列中的每个词元 (token) 生成一个高维度的向量表示，这个向量捕获了该词元在当前句子（或文本片段）中的语义信息。


### 自注意力机制
我们来详细讲解一下自注意力机制（Self-Attention Mechanism）。这是 Transformer 架构（DeBERTa、BERT 等模型的基础）的核心部件，也是让这些模型能够深刻理解文本上下文的关键。

核心思想：

想象一下你在阅读一个句子时，为了理解某个词的意思，你的大脑会不自觉地关注句子中其他相关的词。比如，“河边的银行 நேற்று塌陷了”（The bank by the river collapsed yesterday），你会把“银行”和“河边”联系起来，从而知道它指的是河岸，而不是金融机构。

自注意力机制就是模仿这个过程：在处理一个词（或Token）时，模型能够动态地、有选择地关注输入序列中所有其他词（包括它自己），并根据相关性（重要性）来计算这个词的新表示。这里的“自”（Self）指的是注意力发生在输入序列内部，而不是关注外部信息。
运作方式（概念性解释）：
准备工作 (生成 Q, K, V):
对于输入序列中的每一个词（Token），模型首先会根据它当前的嵌入向量（embedding）生成三个不同的向量：
查询向量 (Query, Q): 代表当前词“主动去查询”相关信息的意图。可以想象成这个词在问：“谁和我相关？”
键向量 (Key, K): 代表当前词“能提供什么信息”的标识或标签。可以想象成这个词在说：“这是我的信息标签，想找相关信息的可以来看。”
值向量 (Value, V): 代表当前词实际包含的“内容”或“意义”。如果一个词被认为相关，它的值向量就会被用来更新查询词的表示。
这三个向量是通过将原始词嵌入乘以三个不同的、可学习的权重矩阵（WQ, WK, WV）得到的。模型在训练中会学习这些矩阵，使得生成的 Q, K, V 最有利于任务。
计算注意力分数 (Attention Scores):
要计算一个词（比如词 A）的新表示，模型会拿着词 A 的 查询向量 (QA)，去和序列中所有词（包括词 A 自己）的 键向量 (K1, K2, ..., Kn) 进行计算。
最常用的计算方式是点积 (Dot Product)。QA 与每个 Ki 的点积结果，就代表了词 i 与词 A 的原始相关性分数。分数越高，初步认为相关性越强。
归一化分数 (Normalization using Softmax):
直接使用原始分数可能不稳定，所以模型会使用 Softmax 函数 对上一步得到的所有分数进行归一化。
Softmax 的作用是将这些分数转换成一组概率值（注意力权重），这些概率值的总和为 1。每个概率值代表了对应词对于理解当前词（词 A）的重要性权重。权重越接近 1，表示越重要；越接近 0，表示越不重要。
加权求和 (Weighted Sum of Values):
模型将上一步得到的注意力权重，分别乘以序列中每个词对应的值向量 (V1, V2, ..., Vn)。
然后，将所有这些加权后的值向量求和。
这个最终的和向量，就是词 A 经过自注意力机制计算后得到的新表示。这个新表示融合了整个序列中所有词的信息，并且是以“注意力权重”为指导、有侧重地融合的。
多头注意力 (Multi-Head Attention):
为了让模型能同时关注来自不同表示子空间的信息（比如，有的注意力头关注语法关系，有的关注语义关系），实际应用中通常使用多头注意力。
这意味着上述的 Q, K, V 生成、注意力计算过程会并行地进行多次（比如 8 次或 12 次），每次使用不同的权重矩阵（WQ, WK, WV）。
得到多个输出向量后，通常会将它们拼接起来，再通过一个线性层进行整合。
自注意力机制与您的模型 ([CLS] Token) 的关系：
在您的模型描述中，提到“通过自注意力机制让 [CLS] 标记逐步融合全文本信息”。这是如何发生的呢？
[CLS] 标记和其他普通词一样，也有自己的 Q, K, V 向量。
在模型的每一层自注意力计算中，[CLS] 的 Q 向量会与所有词（包括它自己和其他实际文本词）的 K 向量计算相关性，得到注意力权重。
然后，[CLS] 的新表示是通过加权求和所有词的 V 向量得到的。
因为这个过程在模型的多个层中重复进行，[CLS] 标记就能一层层地、通过自注意力机制不断地“吸收”和“融合”来自整个输入文本 response_a 或 response_b 的信息。
最终，在 DeBERTa 模型的顶层输出处，[CLS] 标记对应的嵌入向量 embed_a 或 embed_b 就成为了一个能够代表整个输入文本序列信息的聚合表示（句子嵌入）。
总结：
自注意力机制允许模型在处理序列中的每个元素时，都能考虑到序列中所有其他元素的影响，并动态地判断哪些元素更重要。这是它能够捕捉长距离依赖关系、理解复杂上下文的关键。在您的模型中，它使得 [CLS] 标记能够有效地“阅读”整个输入响应，并生成一个浓缩了其整体含义的嵌入向量。


## QKV三个矩阵


好的，我们来详细解释一下在 Transformer 模型（尤其是其核心的注意力机制中）Query (Q)，Key (K) 和 Value (V) 这三个矩阵是如何生成的。

核心思想：线性变换

Q, K, V 三个矩阵的生成本质上是通过对输入数据进行线性变换得到的。这意味着我们将输入向量乘以三个不同的、可学习的权重矩阵。

输入是什么？

在解释 QKV 生成之前，我们首先要明确输入是什么。

1.  词嵌入 (Word Embeddings): 文本序列中的每个词或子词单元首先会被转换成一个固定维度的向量，即词嵌入。
2.  位置编码 (Positional Encodings): 由于 Transformer 的自注意力机制本身不包含序列顺序的信息，我们需要向词嵌入中加入位置编码，以表示词在序列中的位置。

所以，注意力机制的实际输入是词嵌入与位置编码相加后得到的向量序列。我们设这个输入序列为 X，其中 X = [x_1, x_2, ..., x_n]，x_i 是序列中第 i 个词的嵌入向量（已经包含了位置信息），其维度通常被称为 d_model。

Q, K, V 的生成过程

对于输入序列中的每一个向量 x_i，我们通过以下方式生成对应的 q_i, k_i, v_i 向量：

1.  Query (Q) 向量:
    q_i = x_i * W_Q
    这里 W_Q 是一个可学习的权重矩阵，其维度是 d_model × d_k。
    q_i 是第 i 个词的查询向量，维度为 d_k。
    作用：Query 代表当前词，它要去“查询”或“关注”序列中的其他词，看看哪些词与自己更相关。

2.  Key (K) 向量:
    k_i = x_i * W_K
    这里 W_K 是另一个可学习的权重矩阵，其维度也是 d_model × d_k。
    k_i 是第 i 个词的键向量，维度为 d_k。
    作用：Key 代表序列中（包括当前词自身）的每个词的“标识”或“特征”。Query 会与所有的 Key 进行比较（通常通过点积）来计算注意力得分。

3.  Value (V) 向量:
    v_i = x_i * W_V
    这里 W_V 是第三个可学习的权重矩阵，其维度是 d_model × d_v。
    v_i 是第 i 个词的值向量，维度为 d_v。
    作用：Value 代表序列中每个词实际携带的信息或内容。一旦通过 Q 和 K 计算出注意力权重，这些权重就会作用于 Value 向量，对它们进行加权求和，得到最终的输出。

维度说明:

d_model：输入词嵌入的维度。
d_k：Query 和 Key 向量的维度。
d_v：Value 向量的维度。
在原始的 Transformer 论文中，通常有 d_k = d_v = d_model / h，其中 h 是注意力头的数量（见下文“多头注意力”）。但 d_k 和 d_v 也可以独立设置。

矩阵形式:

如果我们将整个输入序列 X 看作一个矩阵 (形状为 sequence_length × d_model)，那么 Q, K, V 也可以表示为矩阵形式：

Q = X * W_Q (形状为 sequence_length × d_k)
K = X * W_K (形状为 sequence_length × d_k)
V = X * W_V (形状为 sequence_length × d_v)

权重矩阵 W_Q, W_K, W_V 的特性：

* 可学习 (Learnable): 这些权重矩阵的参数是在模型训练过程中通过反向传播算法学习得到的。模型会学习如何将输入映射到合适的 Q, K, V 表示，以便更好地捕捉词之间的依赖关系。
* 独立性: W_Q, W_K, W_V 是三个独立的矩阵，它们有各自的参数。
* 目标驱动: 它们学习的目标是使得 Q 和 K 的点积能够准确反映词之间的相关性，并且使得 V 能够提供有用的信息进行加权聚合。

多头注意力 (Multi-Head Attention) 中的 QKV 生成:

在实际的 Transformer 模型中，通常使用多头注意力机制。这意味着我们不仅仅有一组 W_Q, W_K, W_V，而是有 h 组独立的权重矩阵，对应 h 个“注意力头”。

对于每一个头 j (其中 j 从 1 到 h)：

Q_j = X * W_Q^j
K_j = X * W_K^j
V_j = X * W_V^j

这里 W_Q^j, W_K^j, W_V^j 是第 j 个注意力头的权重矩阵。每个头会学习输入数据的不同子空间表示，从而能够从不同角度捕捉信息。

通常，每个头的 d_k 和 d_v 会设置成 d_model / h，这样在将所有头的输出拼接起来时，总维度仍然接近 d_model。

偏置项 (Bias):

在实际实现中，上述线性变换通常还会包含一个偏置项 (bias term)，尽管有时为了简化会被忽略。

q_i = x_i * W_Q + b_Q
k_i = x_i * W_K + b_K
v_i = x_i * W_V + b_V

这些偏置项 b_Q, b_K, b_V 也是可学习的参数。

不同类型的注意力机制中的 QKV:

* 自注意力 (Self-Attention): 这是编码器（Encoder）和解码器（Decoder）内部主要使用的注意力类型。
Q, K, V 都来自于同一个输入序列。例如，在编码器的一个自注意力层中，X 是前一层的输出。
* 交叉注意力 (Cross-Attention) 或 编码器-解码器注意力: 主要用在解码器中，连接编码器和解码器。
    * Q 来自于解码器前一层的输出 (decoder's previous hidden state)。
    * K 和 V 来自于编码器的最终输出 (encoder's output sequence)。
    * 即：Q_decoder = X_decoder * W_Q
    * K_encoder = X_encoder * W_K
    * V_encoder = X_encoder * W_V
    * 这里的 W_Q, W_K, W_V 仍然是这个特定交叉注意力层独立的、可学习的权重矩阵。

总结:

Q, K, V 三个矩阵是通过将输入序列（词嵌入+位置编码）分别乘以三个独立的、可学习的权重矩阵 (W_Q, W_K, W_V) 生成的。
这些矩阵使得模型能够动态地计算序列中不同部分之间的相关性（通过 Q 和 K），并根据这些相关性聚合信息（通过 V），这是 Transformer 模型强大能力的核心机制之一。
在多头注意力中，这个过程会并行地执行多次，每次使用不同的权重矩阵集。

我们用一个非常简化的例子来解释 Q, K, V 矩阵是如何通过数字计算生成的。

**1. 假设我们的输入**

* **一句话很短:** "你好" (Hello you)
* **词嵌入 + 位置编码后的向量 (输入 X):**
    假设经过词嵌入和位置编码后，我们得到每个词的向量表达（维度 d_model）。为了简单，我们设 d_model = 4。
    * x_1 ("你") = `[1, 0, 1, 0]`
    * x_2 ("好") = `[0, 1, 0, 1]`

    所以，输入矩阵 X (形状是 `sequence_length × d_model` = `2 × 4`) 是：
    ```
    X = [[1, 0, 1, 0],   # 你
         [0, 1, 0, 1]]   # 好
    ```

**2. 假设我们的权重矩阵 (这些是模型学习到的)**

* **Q, K 的维度 d_k**: 我们设 d_k = 3
* **V 的维度 d_v**: 我们设 d_v = 2 (V 的维度可以和 Q, K 不同)

* **权重矩阵 W_Q (维度 d_model × d_k = 4 × 3):**
    ```
    W_Q = [[1, 2, 0],
           [0, 1, 1],
           [1, 0, 1],
           [2, 1, 0]]
    ```

* **权重矩阵 W_K (维度 d_model × d_k = 4 × 3):**
    ```
    W_K = [[0, 1, 1],
           [1, 0, 2],
           [2, 1, 0],
           [0, 2, 1]]
    ```

* **权重矩阵 W_V (维度 d_model × d_v = 4 × 2):**
    ```
    W_V = [[1, 0],
           [0, 2],
           [2, 1],
           [1, 1]]
    ```

**3. 计算 Q, K, V 矩阵**

核心公式是：
Q = X * W_Q
K = X * W_K
V = X * W_V

**计算 Q 矩阵 (形状 `2 × 3`):**
Q = X * W_Q
[[1, 0, 1, 0],      [[1, 2, 0],
[0, 1, 0, 1]]   * [0, 1, 1],
[1, 0, 1],
[2, 1, 0]]

* q_1 ("你"的Query向量) = x_1 * W_Q
    = `[1, 0, 1, 0]` * W_Q
    = `[(1*1 + 0*0 + 1*1 + 0*2), (1*2 + 0*1 + 1*0 + 0*1), (1*0 + 0*1 + 1*1 + 0*0)]`
    = `[ (1+0+1+0), (2+0+0+0), (0+0+1+0) ]`
    = `[2, 2, 1]`

* q_2 ("好"的Query向量) = x_2 * W_Q
    = `[0, 1, 0, 1]` * W_Q
    = `[(0*1 + 1*0 + 0*1 + 1*2), (0*2 + 1*1 + 0*0 + 1*1), (0*0 + 1*1 + 0*1 + 1*0)]`
    = `[ (0+0+0+2), (0+1+0+1), (0+1+0+0) ]`
    = `[2, 2, 1]`

所以，Q 矩阵是：
Q = [[2, 2, 1],   # 你 的 Query
[2, 2, 1]]   # 好 的 Query


**计算 K 矩阵 (形状 `2 × 3`):**
K = X * W_K
[[1, 0, 1, 0],      [[0, 1, 1],
[0, 1, 0, 1]]   * [1, 0, 2],
[2, 1, 0],
[0, 2, 1]]

* k_1 ("你"的Key向量) = x_1 * W_K
    = `[1, 0, 1, 0]` * W_K
    = `[(1*0 + 0*1 + 1*2 + 0*0), (1*1 + 0*0 + 1*1 + 0*2), (1*1 + 0*2 + 1*0 + 0*1)]`
    = `[ (0+0+2+0), (1+0+1+0), (1+0+0+0) ]`
    = `[2, 2, 1]`

* k_2 ("好"的Key向量) = x_2 * W_K
    = `[0, 1, 0, 1]` * W_K
    = `[(0*0 + 1*1 + 0*2 + 1*0), (0*1 + 1*0 + 0*1 + 1*2), (0*1 + 1*2 + 0*0 + 1*1)]`
    = `[ (0+1+0+0), (0+0+0+2), (0+2+0+1) ]`
    = `[1, 2, 3]`

所以，K 矩阵是：
K = [[2, 2, 1],   # 你 的 Key
[1, 2, 3]]   # 好 的 Key


**计算 V 矩阵 (形状 `2 × 2`):**
V = X * W_V
[[1, 0, 1, 0],      [[1, 0],
[0, 1, 0, 1]]   * [0, 2],
[2, 1],
[1, 1]]

* v_1 ("你"的Value向量) = x_1 * W_V
    = `[1, 0, 1, 0]` * W_V
    = `[(1*1 + 0*0 + 1*2 + 0*1), (1*0 + 0*2 + 1*1 + 0*1)]`
    = `[ (1+0+2+0), (0+0+1+0) ]`
    = `[3, 1]`

* v_2 ("好"的Value向量) = x_2 * W_V
    = `[0, 1, 0, 1]` * W_V
    = `[(0*1 + 1*0 + 0*2 + 1*1), (0*0 + 1*2 + 0*1 + 1*1)]`
    = `[ (0+0+0+1), (0+2+0+1) ]`
    = `[1, 3]`

所以，V 矩阵是：
V = [[3, 1],   # 你 的 Value
[1, 3]]   # 好 的 Value


**总结一下我们得到的 Q, K, V 矩阵：**

* **Q 矩阵:**
    ```
    [[2, 2, 1],
     [2, 2, 1]]
    ```
* **K 矩阵:**
    ```
    [[2, 2, 1],
     [1, 2, 3]]
    ```
* **V 矩阵:**
    ```
    [[3, 1],
     [1, 3]]
    ```

**这些 Q, K, V 接下来用来做什么？**

这些矩阵是注意力机制的直接输入。简单来说，下一步是：

1.  **计算注意力得分:** 对于每个词的 Query (Q 中的一行)，它会和所有词的 Key (K 中的所有行) 进行点积运算，然后进行缩放 (除以 sqrt(d_k) )，再通过 Softmax 函数得到权重。
    * 例如，"你" (q_1) 对 "你" (k_1) 的关注度，"你" (q_1) 对 "好" (k_2) 的关注度。
    * 同样，"好" (q_2) 对 "你" (k_1) 的关注度，"好" (q_2) 对 "好" (k_2) 的关注度。

2.  **加权求和 Value:** 将计算出来的注意力权重分别乘以对应的 Value 向量 (V 中的行)，然后相加，得到该 Query 词的最终输出向量。

**关键点：**

* W_Q, W_K, W_V 这三个权重矩阵是模型在训练过程中**学习**到的。它们一开始是随机初始化的，通过大量数据训练后，模型会学会如何设置这些权重，使得 Q, K, V 能够有效地捕捉词与词之间的关系。
* 这个例子非常简化，实际的维度会大得多 (例如 d_model=512, d_k=d_v=64)。
* **多头注意力 (Multi-Head Attention):** 实际中，这个过程会并行地进行多次（比如8次，即8个“头”），每次使用不同组的 W_Q, W_K, W_V 权重矩阵。每个“头”可以关注输入序列的不同方面。然后将所有“头”的输出结果拼接起来再进行一次线性变换。这样做可以增强模型的表达能力。在多头注意力中，每个头的 d_k 和 d_v 通常是 d_model 除以头的数量。

希望这个带数字的例子能帮助你更好地理解 Q, K, V 是如何从输入通过可学习的线性变换（矩阵乘法）生成的！


# 知识

