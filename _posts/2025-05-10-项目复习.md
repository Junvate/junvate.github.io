---
layout:     post
title:      项目复习
subtitle:   项目复习
date:       2025-05-08
author:     Junvate
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 保研复习
---


# 项目
## 速记

该项目核心采用了检索增强生成（RAG）技术架构，旨在提升大型语言模型（LLM）在处理专业文档（如技术博客）问答时的准确性和时效性。其技术实现主要包括：利用嵌入模型（Embedding Models）将文档内容转化为向量表示（Vector Embeddings），并存储于向量数据库（Vector Database）中以实现高效索引；当用户提问时，通过语义搜索（Semantic Search）算法（如余弦相似度）在向量库中匹配并检索出最相关的文本片段。接着，关键的整合与生成步骤由 LangChain 框架进行流程编排（Orchestration），它负责将检索到的上下文信息与用户问题组合成有效的提示（Prompt），并调用大型语言模型（LLM，如 GPT-3.5-Turbo 或类似模型）生成回答。同时，LangChain 也被用来实现对话记忆（Memory）和状态管理。最后，项目使用 Ragas 评估框架对 RAG 系统的性能（如答案的忠实度和相关性）进行量化评估，验证了相比基础 LLM 精准度提升 40% 的技术效果。


这个项目旨在预测在特定提示（prompt）下，用户更偏好两个语言模型响应（response_a 或 response_b）中的哪一个，这是一个三分类问题（A赢、B赢或平局）。其核心技术是基于强大的预训练模型 DeBERTa V3 进行模型微调 (Model Fine-tuning)项目首先加载 CSV 数据，使用 eval 处理列表式字符串，填充缺失值，并通过 idxmax 在 winner_model_a, winner_model_b, winner_tie 列上确定优胜者，然后将其映射为数字标签 class_id（0, 1, 2）。关键的预处理步骤是将提示与每个响应按特定格式拼接（使用 f-string 如 f"{prompt}\n\nResponse A:\n{response_a}"），为每个样本生成一对包含上下文的响应文本。接着，使用 KerasNLP 提供的  deberta_v3 将这些文本对 tokenize 成包含 token_ids 和 padding_mask 的字典，其形状均为 [2, 512]。
模型构建阶段 (build_model 函数)：模型接收这个字典输入，在内部分离处理两个序列（prompt+A 和 prompt+B），分别将它们送入 DebertaV3 模型。利用模型的自注意力机制，提取每个序列对应的 [CLS] 输出嵌入 。随后，将这两个 [CLS] 嵌入向量堆叠 (tf.stack) 起来，并通过 GlobalAveragePooling1D 层进行处理（这实际上是平均了两个 [CLS] 嵌入向量）。最后，将这个池化后的向量输入到一个包含3个神经元、使用 Softmax 激活函数的全连接（Dense）输出层，得到三分类的概率预测。训练过程中，模型使用分类交叉熵 (CategoricalCrossentropy) 作为损失函数进行编译，采用了自定义的带预热的余弦衰减学习率调度器 (WarmUpCosineDecay)，并通过 ModelCheckpoint 回调函数监测验证集损失 (val_loss) 来保存性能最佳的模型权重。
从而将 DeBERTa V3的通用语言理解能力适配到这个特定的偏好预测任务上。







## RAG
### 什么是RAG 讲一下工作原理？
RAG（Retrieval-Augmented Generation，检索增强生成）是一种结合了信息检索和生成式语言模型的混合框架，旨在提升大语言模型（LLM）在回答问题时的准确性、相关性和知识时效性。它通过从外部知识库检索相关信息并将其融入生成过程，有效解决LLM的幻觉问题（生成不准确或虚构内容）和知识局限性（无法获取最新或专业知识）。RAG特别适用于需要引用外部文档或处理专业领域知识的场景

RAG的工作原理:  
RAG的核心思想是将信息检索与生成式模型结合，分为两个主要阶段：  
检索阶段：从外部知识库中检索与用户查询最相关的文档或片段，作为生成阶段的上下文。  
生成阶段：将检索到的文档与用户查询结合，输入到生成式语言模型中，生成最终的回答。

## Yolo
### 什么是Yolo
YOLO（You Only Look Once）是一种基于深度学习的目标检测算法，它通过将目标检测任务转化为单一的回归问题，直接从输入图像预测边界框（bounding box）和类别概率，实现了端到端的检测。

网格划分：YOLO将输入图像划分为S×S的网格，每个网格负责检测其覆盖区域内的目标。  
预测内容：每个网格预测多个边界框（包括坐标、宽高）、置信度（表示框内是否有目标）以及类别概率。  
端到端训练：通过卷积神经网络（CNN）一次性处理整个图像，输出所有检测结果。  
后处理：使用非极大值抑制（NMS）去除重叠框，保留最佳检测结果。





### 你提到mAP50指标达到91.4%，请解释mAP50的计算方法及其在目标检测中的意义。
mAP50是目标检测中的重要评估指标，表示在IoU（交并比）阈值为0.5时的平均精度。
对每个类别，计算预测结果的精度-召回曲线（Precision-Recall Curve）。
计算每条曲线的曲线下面积（AUC）
对所有类别的AUC取平均值，得到mAP50。mAP50的意义在于衡量模型在不同类别上的检测性能，91.4%表明模型在IoU≥0.5时具有较高的准确性。它是评估目标检测模型精度的重要标准，尤其适用于实时应用场景

## DeBERTa V3
### DeBERTa V3是什么
- DeBERTa V3 是由微软研究院开发的一种预训练自然语言处理（NLP）模型。

- DeBERTa V3 是一个通过采用更高效的 ELECTRA 式预训练方法（RTD）显著提升了训练效率和性能的 Transformer 语言模型。它在自然语言理解任务上表现强大

DeBERTa V3 模型会对输入的 token IDs 和 padding mask 进行嵌入（embedding）、位置编码（positional encoding）和多层自注意力机制（self-attention）等操作，最终生成每个 token 的上下文感知的嵌入表示

backbone 的作用: DebertaV3Backbone 这样的模型通常用来将输入的 token ID 序列转换成上下文相关的词嵌入 (contextualized embeddings) 或称为隐藏状态 (hidden states)。简单来说，它为输入序列中的每个词元 (token) 生成一个高维度的向量表示，这个向量捕获了该词元在当前句子（或文本片段）中的语义信息。


### 自注意力机制
我们来详细讲解一下自注意力机制（Self-Attention Mechanism）。这是 Transformer 架构（DeBERTa、BERT 等模型的基础）的核心部件，也是让这些模型能够深刻理解文本上下文的关键。

核心思想：

想象一下你在阅读一个句子时，为了理解某个词的意思，你的大脑会不自觉地关注句子中其他相关的词。比如，“河边的银行 நேற்று塌陷了”（The bank by the river collapsed yesterday），你会把“银行”和“河边”联系起来，从而知道它指的是河岸，而不是金融机构。

自注意力机制就是模仿这个过程：在处理一个词（或Token）时，模型能够动态地、有选择地关注输入序列中所有其他词（包括它自己），并根据相关性（重要性）来计算这个词的新表示。这里的“自”（Self）指的是注意力发生在输入序列内部，而不是关注外部信息。
运作方式（概念性解释）：
准备工作 (生成 Q, K, V):
对于输入序列中的每一个词（Token），模型首先会根据它当前的嵌入向量（embedding）生成三个不同的向量：
查询向量 (Query, Q): 代表当前词“主动去查询”相关信息的意图。可以想象成这个词在问：“谁和我相关？”
键向量 (Key, K): 代表当前词“能提供什么信息”的标识或标签。可以想象成这个词在说：“这是我的信息标签，想找相关信息的可以来看。”
值向量 (Value, V): 代表当前词实际包含的“内容”或“意义”。如果一个词被认为相关，它的值向量就会被用来更新查询词的表示。
这三个向量是通过将原始词嵌入乘以三个不同的、可学习的权重矩阵（WQ, WK, WV）得到的。模型在训练中会学习这些矩阵，使得生成的 Q, K, V 最有利于任务。
计算注意力分数 (Attention Scores):
要计算一个词（比如词 A）的新表示，模型会拿着词 A 的 查询向量 (QA)，去和序列中所有词（包括词 A 自己）的 键向量 (K1, K2, ..., Kn) 进行计算。
最常用的计算方式是点积 (Dot Product)。QA 与每个 Ki 的点积结果，就代表了词 i 与词 A 的原始相关性分数。分数越高，初步认为相关性越强。
归一化分数 (Normalization using Softmax):
直接使用原始分数可能不稳定，所以模型会使用 Softmax 函数 对上一步得到的所有分数进行归一化。
Softmax 的作用是将这些分数转换成一组概率值（注意力权重），这些概率值的总和为 1。每个概率值代表了对应词对于理解当前词（词 A）的重要性权重。权重越接近 1，表示越重要；越接近 0，表示越不重要。
加权求和 (Weighted Sum of Values):
模型将上一步得到的注意力权重，分别乘以序列中每个词对应的值向量 (V1, V2, ..., Vn)。
然后，将所有这些加权后的值向量求和。
这个最终的和向量，就是词 A 经过自注意力机制计算后得到的新表示。这个新表示融合了整个序列中所有词的信息，并且是以“注意力权重”为指导、有侧重地融合的。
多头注意力 (Multi-Head Attention):
为了让模型能同时关注来自不同表示子空间的信息（比如，有的注意力头关注语法关系，有的关注语义关系），实际应用中通常使用多头注意力。
这意味着上述的 Q, K, V 生成、注意力计算过程会并行地进行多次（比如 8 次或 12 次），每次使用不同的权重矩阵（WQ, WK, WV）。
得到多个输出向量后，通常会将它们拼接起来，再通过一个线性层进行整合。
自注意力机制与您的模型 ([CLS] Token) 的关系：
在您的模型描述中，提到“通过自注意力机制让 [CLS] 标记逐步融合全文本信息”。这是如何发生的呢？
[CLS] 标记和其他普通词一样，也有自己的 Q, K, V 向量。
在模型的每一层自注意力计算中，[CLS] 的 Q 向量会与所有词（包括它自己和其他实际文本词）的 K 向量计算相关性，得到注意力权重。
然后，[CLS] 的新表示是通过加权求和所有词的 V 向量得到的。
因为这个过程在模型的多个层中重复进行，[CLS] 标记就能一层层地、通过自注意力机制不断地“吸收”和“融合”来自整个输入文本 response_a 或 response_b 的信息。
最终，在 DeBERTa 模型的顶层输出处，[CLS] 标记对应的嵌入向量 embed_a 或 embed_b 就成为了一个能够代表整个输入文本序列信息的聚合表示（句子嵌入）。
总结：
自注意力机制允许模型在处理序列中的每个元素时，都能考虑到序列中所有其他元素的影响，并动态地判断哪些元素更重要。这是它能够捕捉长距离依赖关系、理解复杂上下文的关键。在您的模型中，它使得 [CLS] 标记能够有效地“阅读”整个输入响应，并生成一个浓缩了其整体含义的嵌入向量。



# 知识

