---
layout:     post
title:      项目复习
subtitle:   项目复习
date:       2025-05-10
author:     Junvate
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 保研复习
---


# 项目
## 速记
针对现有目标检测模型在处理工地场景中常见的小目标（如远距离工人）或因光照、遮挡导致的低对比度目标时，普遍存在关键特征提取不足、对目标区域聚焦不够精准的共性瓶颈，这直接影响了安全监控系统的实际检测精度与可靠性。为解决这一挑战，我独立开发并成功部署了一套基于YOLOv10的智慧工地安全监控系统，旨在通过实时智能监测工人安全帽和反光衣的佩戴情况，显著提升建筑工地的安全管理效率和主动预警水平。
在该项目中，我负责了从系统构思、前期工地特定场景下的大规模数据采集、精细化清洗与标注，到核心的YOLOv10目标检测模型的构建、训练以及关键的深度优化全过程。为有效攻克小目标检测难题并提升整体检测性能，我的核心优化工作包括：
引入EIoU损失函数，以提升对安全帽、反光衣等目标边界框的定位精度，减少漏检和误检。
通过在不同尺度的特征层上分别审慎部署SimAM无参数注意力模块，以增强模型对关键特征（尤其是在复杂背景或目标部分遮挡条件下）的提取与辨识能力。
这些深度优化与针对性改进措施，在自建的、包含大量真实工地复杂小目标场景的测试集上取得了显著成效：模型的mAP50指标从优化前的基线85.7%成功提升至91.4%。这一性能不仅验证了优化策略的有效性，也大幅超越了在该数据集上进行评估的YOLOv8（其mAP50为82.3%）和Fast R-CNN（其mAP50为84.6%）这两款代表性模型。
在系统实现层面，我还利用OpenCV高效完成了视频流的实时捕获、解码及图像预处理工作，并基于检测结果开发了与之联动的自动告警系统。最终，我负责将整套系统成功部署到生产环境，通过持续的监控与维护，确保了其7x24小时的稳定运行和及时有效的预警功能，为提升工地安全管理水平提供了坚实的技术保障。

这个项目旨在预测在特定提示（prompt）下，用户更偏好两个语言模型响应（response_a 或 response_b）中的哪一个，这是一个三分类问题（A赢、B赢或平局）。其核心技术是基于强大的预训练模型 DeBERTa V3 进行模型微调 (Model Fine-tuning)项目首先加载 CSV 数据，使用 eval 处理列表式字符串，填充缺失值，并通过 idxmax 在 winner_model_a, winner_model_b, winner_tie 列上确定优胜者，然后将其映射为数字标签 class_id（0, 1, 2）。关键的预处理步骤是将提示与每个响应按特定格式拼接（使用 f-string 如 f"{prompt}\n\nResponse A:\n{response_a}"），为每个样本生成一对包含上下文的响应文本。接着，使用 KerasNLP 提供的  deberta_v3 将这些文本对 tokenize 成包含 token_ids 和 padding_mask 的字典，其形状均为 [2, 512]。
模型构建阶段 (build_model 函数)：模型接收这个字典输入，在内部分离处理两个序列（prompt+A 和 prompt+B），分别将它们送入 DebertaV3 模型。利用模型的自注意力机制，提取每个序列对应的 [CLS] 输出嵌入 。随后，将这两个 [CLS] 嵌入向量堆叠 (tf.stack) 起来，并通过 GlobalAveragePooling1D 层进行处理（这实际上是平均了两个 [CLS] 嵌入向量）。最后，将这个池化后的向量输入到一个包含3个神经元、使用 Softmax 激活函数的全连接（Dense）输出层，得到三分类的概率预测。训练过程中，模型使用分类交叉熵 (CategoricalCrossentropy) 作为损失函数进行编译，采用了自定义的带预热的余弦衰减学习率调度器 (WarmUpCosineDecay)，并通过 ModelCheckpoint 回调函数监测验证集损失 (val_loss) 来保存性能最佳的模型权重。
从而将 DeBERTa V3的通用语言理解能力适配到这个特定的偏好预测任务上。

该项目核心采用了检索增强生成（RAG）技术架构，旨在提升大型语言模型（LLM）在处理专业文档（如技术博客）问答时的准确性和时效性。其技术实现主要包括：利用嵌入模型（Embedding Models）将文档内容转化为向量表示（Vector Embeddings），并存储于向量数据库（Vector Database）中以实现高效索引；当用户提问时，通过语义搜索（Semantic Search）算法（如余弦相似度）在向量库中匹配并检索出最相关的文本片段。接着，关键的整合与生成步骤由 LangChain 框架进行流程编排（Orchestration），它负责将检索到的上下文信息与用户问题组合成有效的提示（Prompt），并调用大型语言模型（LLM，如 GPT-3.5-Turbo 或类似模型）生成回答。同时，LangChain 也被用来实现对话记忆（Memory）和状态管理。最后，项目使用 Ragas 评估框架对 RAG 系统的性能（如答案的忠实度和相关性）进行量化评估，验证了相比基础 LLM 精准度提升 40% 的技术效果。





## yolo
### 通过**引入EIoU损失函数**提升边界框精度?
- 回答：

之所以选择EIoU，是因为对于小目标，由于其本身尺寸极小，任何微小的绝对尺寸偏差都会被放大，对检测效果产生巨大影响。EIoU通过直接关注并惩罚这些绝对的宽度和高度差异，解耦了对这两个维度的优化，从而为模型提供了更敏感（对微小差异反应强烈）和更直接（梯度路径清晰）的指导信号。这使得模型能够更有效地学习如何将预测框的边界精确地对齐到小目标的少数几个像素上，从而显著提升小目标的定位精度和整体检测性能。相比之下，仅关注宽高比一致性的损失函数（如CIoU中的相应项）在面对小目标时，可能因为比例碰巧正确而忽略了关键的绝对尺寸差异，或者提供的梯度信号不够强劲和直接，导致模型在精细调整小目标边界时效果欠佳。


IoU通过其直接且解耦的宽高惩罚机制，使得模型在面对这种低容错、高精度需求的小目标时，能够获得更精细、更有效的梯度来优化边界框的每一个像素，从而显著提升定位精度。模型不再仅仅是学习一个“大致正确的宽高比”，而是在学习“精确的宽度值”和“精确的高度值”。

- 1. 目标是什么？边界框回归 (Bounding Box Regression)

在目标检测任务中，模型需要为识别出的每个物体画一个紧密的框（即“边界框”）。“边界框回归”指的是精确调整这个预测框的位置（中心点的x, y坐标）和尺寸（宽度、高度），使其尽可能地与真实物体的位置和大小（即“真实框”或“Ground Truth Box”）相匹配。

- 2. 什么是损失函数 (Loss Function)？

损失函数是训练机器学习模型时至关重要的一部分。它衡量模型预测结果与真实情况之间的“差距”或“错误程度”。在训练过程中，模型会不断尝试最小化这个损失值。通常，损失值越低，意味着模型的预测越准确。

作为损失函数 (IoU Loss)： 可以直接将 IoU 用作损失函数（例如，L IoU=1−IoU）。目标是最大化 IoU，因此最小化 1−IoU 就能达到这个目的。

- IoU Loss 的局限性：
无重叠框的梯度消失问题： 如果预测框与真实框完全没有重叠，IoU 值为 0。此时，IoU 损失为 1，但其梯度（用于更新模型的信号）会变为零。这意味着模型无法从损失中获取任何关于如何移动预测框以使其与真实框重叠的信息。这就像在黑暗中迷路，没有任何方向指引。
无法区分不同类型的未对齐： 如果两个预测框与真实框的 IoU 值相同，IoU 损失无法区分它们未对齐的具体方式。例如，一个预测框可能离真实框很远但形状相似，而另一个预测框可能离真实框较近但宽高比差异很大，IoU 损失会给予它们相似的惩罚。

- EIoU (Enhanced/Efficient Intersection over Union) 损失函数

EIoU 损失在基础 IoU 的上，通过增加额外的惩罚项来解决普通 IoU 损失的局限性，旨在使边界框回归过程更高效、更准确。

![](/img/eiou.png)
![](/img/ciou.png)
![](/img/ciou_eiou.png)
![](/img/优势.png)
## 什么是注意力模块 什么是无参数注意力模块 什么是simam 这里为什么要用这个 有什么用？

- 什么是注意力模块 (Attention Module)？
注意力模块是深度学习（尤其是在计算机视觉和自然语言处理领域）中一种模仿人类认知注意力的组件或机制。

核心思想：并非输入数据的所有部分都同等重要。注意力模块允许神经网络在处理信息时，动态地、有选择性地关注输入数据中更相关的部分，同时抑制不太相关的部分。

- 什么是无参数注意力模块 (Parameter-Free Attention Module)？
无参数注意力模块是一种特殊类型的注意力模块，它在计算注意力权重时不需要引入额外的可学习参数（例如，不需要额外的全连接层或卷积层来学习如何生成注意力）。

- 什么是SimAM (Simple, Parameter-Free Attention Module)？
SimAM 是一种新颖的、简单且无参数的注意力模块。它的设计灵感来源于神经科学中的一些发现，特别是关于神经元信息处理的理论。

核心思想：SimAM旨在通过评估每个神经元（或特征图中的每个激活值）与其周围神经元之间的线性和非线性可分性来衡量其重要性。它假设那些与其他神经元表现出不同激活模式的神经元（即信息量更丰富的神经元）应该被赋予更高的注意力。


SimAM的核心思想源于神经科学，即信息量丰富的神经元通常会表现出与周围神经元不同的激活模式，并且会抑制其他神经元的活动。为了在神经网络中模拟这一点，SimAM为特征图中的每一个神经元（或激活值）定义了一个能量函数 (Energy Function)。能量越低的神经元被认为越重要，因此会获得更高的注意力权重。
![](/img/simam.png)

![](/img/simam公式作用.png)

## RAG
### 什么是RAG 讲一下工作原理？
RAG（Retrieval-Augmented Generation，检索增强生成）是一种结合了信息检索和生成式语言模型的混合框架，旨在提升大语言模型（LLM）在回答问题时的准确性、相关性和知识时效性。它通过从外部知识库检索相关信息并将其融入生成过程，有效解决LLM的幻觉问题（生成不准确或虚构内容）和知识局限性（无法获取最新或专业知识）。RAG特别适用于需要引用外部文档或处理专业领域知识的场景

RAG的工作原理:  
RAG的核心思想是将信息检索与生成式模型结合，分为两个主要阶段：  
检索阶段：从外部知识库中检索与用户查询最相关的文档或片段，作为生成阶段的上下文。  
生成阶段：将检索到的文档与用户查询结合，输入到生成式语言模型中，生成最终的回答。


### 核心创新：内容感知型文档切分策略?
- 技术文档，比如我们处理的技术博客、API文档或研究论文，通常具有非常清晰的结构层次。我们的策略会智能地识别这些结构元素，例如：一级、二级、三级标题，代码块，列表项，表格，甚至是图注等。
在切分时，它会优先尊重这些结构边界
- 除了显式结构，文档内容本身也存在内在的语义流和主题连贯性。我们的策略会努力捕捉这种语义逻辑。
例如，它会分析段落之间或句子之间的主题连贯性，
- 而我们的策略，通过上述对结构和语义的感知，能够生成在语义上更完整、上下文保留更完好、信息也更集中的知识块。

### 你的向量数据库  和 word2vec分别是什么模型？

- text-embedding-3-large 是OpenAI 公司最新一代（截至目前信息）的文本嵌入模型中规模较大、性能较强的一款。
- text-embedding-3-large 基于 Transformer 架构（与GPT系列模型类似的基础架构），经过大量文本数据的训练，学会了如何将输入的文本序列编码成一个固定维度3072的稠密向量。这个向量能够有效地表征原始文本的核心意义。





### 你这个评估框架Ragas是怎么进行评估的？
- Ragas框架提供多个衡量评价RAG系统的指标包括
- failfulness忠实度：是否是基于上下文进行回答的，这是衡量模型是否产生幻觉的一个标准
- answer relevance相关度：回答的问题是否和问题相关，它会评估答案是否集中于解决问题的核心。
- context precision上下文精准度:衡量所有检索到的上下文中，和答案真正有关系的比例
- context recall上下文召回率：衡量所有与答案有关的上下文中，有多少被检索系统成功找回来


### 语义相似性从用户提供的网页文档中检索信息”，具体是如何实现的？使用了什么嵌入模型（Embedding Model）将文本转换为向量？为什么选择这款嵌入模型？
从用户提供的网页文档中通过语义相似性检索信息时，
其实现过程通常是：首先，系统会抓取并解析网页内容，提取出主要的文本信息，去除HTML标签、广告、导航栏等无关元素；接着，这些文本会被送入一个嵌入模型（Embedding Model）转换为高维向量。例如，我可能会选择使用像 text-embedding-3-small (来自OpenAI) 这样的先进模型，
或者一个强大的开源模型如 sentence-transformers/all-mpnet-base-v2。选择这类模型的原因在于它们在各类语义理解任务上表现出色，能够生成捕捉文本深层含义的向量，并且在性能、成本（对于API模型）和易用性之间取得了良好平衡，确保了后续相似性搜索的准确性。


### 检索时，你是如何定义和衡量“语义相似性”的？通常会检索多少个相关的文本块（top-k）？这个k值是如何确定的？
在检索时，“语义相似性”通常是通过计算查询文本向量与文档块向量之间的余弦相似度 (Cosine Similarity) 来定义和衡量的；
对于归一化后的向量，内积也可以达到同样的效果。
余弦相似度能够很好地捕捉向量方向上的一致性，即语义上的接近程度，而不受向量长度的影响。
通常会检索 top-k 个最相关的文本块，k 的值一般会根据经验和实验来确定，常见的范围是 3到5个。
这个k值的确定需要权衡多个因素：一方面要确保提供给LLM足够的信息以回答问题，
另一方面要避免引入过多不相关或冗余的信息，这会增加LLM的处理负担并可能降低答案质量。
通过在验证集上测试不同的k值，并评估最终问答的准确率、Ragas指标（如Faithfulness, Context Recall）等，
可以找到一个最优的k值。


### 如果让你继续优化这个系统，你有哪些改进的想法？
- 引入更先进的检索策略： 最重要的一点是实现**重排序 (Re-ranking)**机制，即在初步向量检索后，使用更强大的交叉编码器（cross-encoder）对候选文档块进行二次排序，以大幅提升最终送入LLM的上下文的精准度。
- 一个合适的开源LLM进行领域适配微调 (Domain-specific Fine-tuning)，使其更好地理解专业术语，能更好的减少幻觉，但是不无法改进时效性
- 优化Prompt工程： 最重要的一点是为LLM提供极其明确和针对性的指令 (Instruction Tuning for Prompts)
- 支持更复杂的查询： 最重要的一点是实现查询分解 (Query Decomposition)，即利用LLM将复杂的多意图或多子问题查询拆解成更简单、可独立检索的子查询




## Yolo
### 什么是Yolo
- YOLO（You Only Look Once）是一种基于深度学习的目标检测算法，它通过将目标检测任务转化为单一的回归问题，
直接从输入图像预测边界框（bounding box）和类别概率，实现了端到端的检测。

网格划分：YOLO将输入图像划分为S×S的网格，每个网格负责检测其覆盖区域内的目标。  
预测内容：每个网格预测多个边界框（包括坐标、宽高）、置信度（表示框内是否有目标）以及类别概率。  
端到端训练：通过卷积神经网络（CNN）一次性处理整个图像，输出所有检测结果。  
后处理：使用非极大值抑制（NMS）去除重叠框，保留最佳检测结果。

### 为什么你要选择yolo而不是其他算法
- yolov10是一个基于深度学习的单阶段目标检测算法，在**速度检测和效率**上更有优势，对于这种实时处理的部署场景有更大的优势，
- 相对于第八代和第九代，通过无NMS和架构优化优先考虑端到端的实时性能，更注重极致的效率
- 相对于Faster R-CNN ,它是一种双阶段目标检测器，首先进行区域提议（region proposal），然后再对这些提议进行分类和边界框回归
Faster R-CNN以其较高的检测精度闻名，尤其是在处理小目标或复杂场景时，但是yolov10在速度和精度的平衡上会更好
YOLOv10的NMS-free特性进一步拉大了速度差距。
##### 介绍一下NMS
在传统的目标检测中对一个物体的检测常常有多个候选框(bounding boxes)，会有置信度分数，NMS的作用就是**筛选最佳的候选框**
**抑制冗余的候选框**，但是NMS是一个计算的开销增加推理时间
##### 介绍一下NMS-free
- 核心思想是 改进的标签分配策略（Label Assignment）







### 你提到mAP50指标达到91.4%，请解释mAP50的计算方法及其在目标检测中的意义。
- 它的全称是 "mean Average Precision at an IoU threshold of 0.5
- mAP50是目标检测中的重要评估指标，表示在IoU（交并比）阈值为0.5时的平均精度。
- 对每个类别，计算预测结果的精度-召回曲线（Precision-Recall Curve）。计算每条曲线的曲线下面积（AUC）对所有类别的AUC取平均值，得到mAP50。
- mAP50的意义在于衡量模型在不同类别上的检测性能，91.4%表明模型在IoU≥0.5时具有较高的准确性。
- precision精度 衡量的是所有被模型检测出来的有多少是正例
- recall召回率 衡量得失有多少真正的正例被检测出来
- Average Precision就是这条 P-R 曲线下的面积。高 Precision 低 Recall 意味着模型检测到的结果大多是正确的，但漏掉了很多物体。高 Recall 低 Precision 意味着模型检测出了大部分物体，但很多检测结果是错误的。
- F1-score  是precision和recall的调和平均数 = (Precision×Recall)/(Precision+Recall)  它在两者都较高时才会取得高分。

## 你了解AI agent吗
- 感知 (Perception)：
- 思考/决策 (Thinking/Decision Making)：**核心**
目标理解： Agent 首先需要理解被赋予的目标或用户提出的请求。
知识与推理： Agent 可能会访问其内部知识库（训练数据中学习到的知识）或外部知识源（如通过API调用的数据库、搜索引擎等）。它利用这些知识进行推理、规划和决策。
规划 (Planning)： 对于复杂任务，Agent 需要将其分解成一系列更小、可管理的步骤或子任务。它会规划出达成目标的路径，决定先做什么后做什么。例如，著名的 ReAct (Reason + Act) 框架就是让LLM先进行“思考”（生成推理步骤），然后再决定采取哪个“行动”。
工具选择 (Tool Selection)： 现代 Agent 通常被赋予使用多种“工具”的能力。这些工具可以是调用外部API（如天气查询、股票信息、计算器、代码解释器、搜索引擎）、执行特定函数或与其他系统交互。Agent 需要根据当前任务和规划，智能地选择合适的工具。
- 行动 (Action)：
- 记忆 (Memory)：

```
一个基于LLM的Agent的工作流程可能是这样的：
用户给出指令/目标（例如：“帮我查一下明天北京的天气，并预订一家评价不错的意大利餐厅”）。
Agent (LLM核心) 理解目标，并将其分解为子任务：
获取北京明天的天气。
搜索评价好的意大利餐厅。
执行预订操作。
规划并选择工具：
对于天气，选择调用“天气查询API”。
对于餐厅，选择调用“餐厅搜索API”或“网页浏览工具”。
action执行动作 (调用工具)：
调用天气API，输入“北京”、“明天”。
调用餐厅搜索API，输入“意大利菜”、“评价好”。
观察结果：
获取到天气信息。
获取到餐厅列表和评价。
进一步思考和行动：
如果需要预订，可能需要用户确认或提供更多信息，然后调用“预订API”。
整合所有信息。
生成回复： 将天气情况和餐厅信息（或预订结果）以自然语言形式回复给用户。
```




## DeBERTa V3
### Interview Answer: Why DeBERTaV3?
"我们选择 DeBERTaV3 作为我们LLM偏好预测模型框架的基础，主要基于以下几个关键考量，这些考量都指向了它在理解文本细微差异和捕捉上下文逻辑一致性方面的强大能力，这对于准确预测人类偏好至关重要：

卓越的语义理解和推理能力：

DeBERTaV3 引入了增强的掩码解码器 (Enhanced Mask Decoder - EMD) 和梯度不相关注意力 (Gradient-disentangled Attention) 等创新机制。这些技术使其在各种自然语言理解 (NLU) 基准测试中都表现出色，特别是在需要深层语义理解和复杂推理的任务上。
对于我们的偏好预测任务，模型需要精确理解问题（Prompt）的意图以及两个待比较回答（Answers）的细微语义差别，并判断哪个回答与问题的逻辑更一致、质量更高。DeBERTaV3 在这方面的强大基础能力是我们选择它的首要原因。
对上下文和词间关系的精细捕捉：

DeBERTaV3 的设计特别关注于词与词之间的相对位置信息和内容的绝对位置信息。它的梯度不相关注意力机制改进了标准自注意力层中内容和位置嵌入的纠缠问题，使得模型能更清晰地区分和利用这两种信息。
在我们的“双通道对比学习框架”中，这种能力尤为重要。我们需要模型不仅理解每个回答自身的内容，还要细致比较两个回答之间、以及每个回答与原始问题之间的复杂关联和逻辑一致性。DeBERTaV3 对上下文的精细建模为我们实现“特征融合与上下文感知技术”提供了坚实的基础。
效率和性能的平衡：

虽然DeBERTaV3是一个强大的模型，但相较于一些更大参数量的模型，它在性能和效率之间取得了较好的平衡。特别是其较小版本（如DeBERTaV3-base或DeBERTaV3-small，如果适用的话）在保持强大理解能力的同时，也能在合理的计算资源下进行有效的训练和推理。
考虑到我们需要处理包含10万对样本的数据集，并追求评估效率，模型的这种平衡性也是一个重要因素。
预训练模型的成熟度和可用性：

DeBERTaV3 拥有高质量的预训练权重，这些权重是在大规模、多样化的语料上训练得到的。使用这样强大的预训练模型作为起点，可以让我们在特定任务（如偏好预测）上进行微调时，更快地达到较好的性能，并且能更好地泛化。
与对比学习框架的契合度：

DeBERTaV3 作为编码器，能够为我们的“双通道对比学习框架”中的每个回答（Answer A 和 Answer B）生成高质量的上下文感知表示（embeddings）。这些高质量的表示是对比学习成功的关键，因为它们使得模型能够更有效地学习区分“更优”回答和“次优”回答的细微特征。
总而言之，选择DeBERTaV3是因为它在语义理解深度、上下文关系捕捉、性能效率均衡以及预训练模型质量等多个方面都非常契合我们构建先进LLM偏好预测模型的需求，为我们后续的优化工作（如参数共享、特征融合）提供了一个非常强大的起点，并最终帮助我们实现了0.75的Spearman相关性。"
### DeBERTa V3是什么
- DeBERTa V3 是由微软研究院开发的一种预训练自然语言处理（NLP）模型。

- DeBERTa V3 是一个通过采用更高效的 ELECTRA 式预训练方法（RTD）显著提升了训练效率和性能的 Transformer 语言模型。它在自然语言理解任务上表现强大

DeBERTa V3 模型会对输入的 token IDs 和 padding mask 进行嵌入（embedding）、位置编码（positional encoding）和多层自注意力机制（self-attention）等操作，最终生成每个 token 的上下文感知的嵌入表示

backbone 的作用: DebertaV3Backbone 这样的模型通常用来将输入的 token ID 序列转换成上下文相关的词嵌入 (contextualized embeddings) 或称为隐藏状态 (hidden states)。简单来说，它为输入序列中的每个词元 (token) 生成一个高维度的向量表示，这个向量捕获了该词元在当前句子（或文本片段）中的语义信息。



# 知识

