# 面试题
## CNN 如何给图像分类（整体流程）
- 输入 (Input):     
    - 一张图像，通常表示为一个像素值的矩阵，RGB三个矩阵。

- 特征提取 (Feature Extraction):
    - 图像通过卷积层，激活函数，池化层
    - 卷积层负责检测图像的边缘，纹理，角点
    - 激活函数引入非线性，学习复杂模式
    - 池化层降低数据空间维度

- 展平 (Flatten)
    - 经过feature exraction之后，会得到 **通道数个** width*heigth的矩阵，会被展平成一个长长的一维向量

- 分类 (Classification)
    - 一维特征被送入全连接层，根据提取到的特征进行分类
- 输出（output）
    - 通过全连接层的输出之后得到多个类别的得分
    - 然后通过一个 softmax() 得到概率向量

## 卷积层是怎么工作的

- 是卷积神经网络的关键，从输入数据之中提取特征
- 

## 交叉熵损失 cross entropy loss
- cross entropy loss  是分类问题中常用的损失函数，衡量模型预测的概率分布与真实标签的差异，关注的是真实类别预测的概率

$$L = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)$$

$$L = -y_k \log(\hat{y}_k) = -1 \cdot \log(\hat{y}_k) = -\log(\hat{y}_k)$$

# 深度神经网络模型
## 卷积神经网络 convolutional neural networks
- 主要用于处理网格状数据


#### LeNet-5
-  最早的成功 CNN 之一，用于手写数字识别。

#### AlexNet
- 在 2012 年 ImageNet 竞赛中取得突破，真正点燃了深度学习在图像领域的应用。

#### ResNet (Residual Network)
- 引入残差连接，解决了极深网络训练困难（梯度消失/爆炸）的问题，是目前应用最广泛的 CNN 架构之一。


## 循环神经网络 recurrent neural networks
- 主要用于处理序列数据，如文本、语音、时间序列

#### Simple RNN
- 基本的 RNN 结构，但存在长期依赖问题（梯度消失/爆炸）。


#### LSTM 
- (Long Short-Term Memory): 通过引入门控机制（输入门、遗忘门、输出门）有效解决了长期依赖问题，是处理序列数据的里程碑模型。

#### GRU 
- (Gated Recurrent Unit): LSTM 的一种简化变体，结构更简单，性能与 LSTM 相当，在某些任务上更受欢迎。


## Transformer模型
- 为自然语言处理设计，基于(Self-Attention)，完全摒弃了 RNN 的循环结构和 CNN 的卷积操作。

#### Transformer 
- (Original): 论文 "Attention Is All You Need" 中提出的基础架构，成为现代 NLP 的基石。

#### BERT 
- (Bidirectional Encoder Representations from Transformers  基于 Transformer 的双向编码器表示): 基于 Transformer 的预训练语言模型，通过双向上下文理解，在多项 NLP 任务中取得 SOTA (State-of-the-art) 效果。


#### GPT 
- (Generative Pre-trained Transformer) 系列 (e.g., GPT-3, GPT-4): 基于 Transformer 的大规模生成式预训练模型，在文本生成、对话等方面能力强大。